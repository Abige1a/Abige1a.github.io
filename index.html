<!--Skip to content-->
<!--Search or jump to…-->
<!--Pull requests-->
<!--Issues-->
<!--Marketplace-->
<!--Explore-->

<!--@SherwinDengxiong-->
<!--SherwinDengxiong-->
<!--/-->
<!--SherwinDengxiong.github.io-->
<!--Public-->
<!--Code-->
<!--Issues-->
<!--Pull requests-->
<!--Actions-->
<!--Projects-->
<!--Wiki-->
<!--Security-->
<!--Insights-->
<!--Settings-->
<!--SherwinDengxiong.github.io/index.html-->
<!--@SherwinDengxiong-->
<!--SherwinDengxiong Add files via upload-->
<!--Latest commit b13791d on 1 Apr-->
<!--History-->
<!--1 contributor-->
<!--1050 lines (916 sloc)  71.4 KB-->

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Xueting Wang's Homepage</title>
    <style type="text/css">
        <!--
        .STYLE8 {
            font-family: Georgia, "Times New Roman", Times, serif;
            font-size: 32px;
            font-style: italic;
            color: #000033;
        }
        body,td,th {
            font-family: Times New Roman, Times, serif;
            font-size: 18px;
        }
        .STYLE17 {font-family: Georgia, "Times New Roman", Times, serif}
        .STYLE18 {font-size: 18px}
        .STYLE34 {font-size: 16px}
        .STYLE35 {color: #CCCCCC}
        .STYLE75 {color: #0000FF}
        .STYLE80 {color: #000000}
        .STYLE85 {font-family: "Times New Roman", Times, serif; color: #000066; font-weight: bold; font-size: 22px; }
        a:link {
            text-decoration: none;
            color: #0099FF;
        }
        a:visited {
            text-decoration: none;
        }
        a:hover {
            text-decoration: none;
            color: #009933;
        }
        a:active {
            text-decoration: none;
        }
        .STYLE90 {color: #FF0000; font-weight: bold; }
        .STYLE98 {
            color: #333333;
            font-weight: bold;
            font-size: 14px;
        }
        .STYLE100 {color: #CC0033; font-size: 20px; font-style: italic; font-family: "Times New Roman", Times, serif;}
        .STYLE109 {
            font-size: 18pt;
            color: #CC0033;
        }
        .STYLE122 {
            color: #F91F06;
            font-size: 16px;
        }
        .STYLE132 {color: #0066FF}
        .STYLE136 {font-size: 22pt}
        .STYLE137 {font-weight: bold; color: #333333;}
        .STYLE141 {color: #0000CC; font-weight: bold; }
        .STYLE157 {
            color: green;
            font-weight: bold;
        }
        .STYLE162 {font-family: Arial, Helvetica, sans-serif; font-size: 14px; font-style: italic; }
        .STYLE178 {color: green; font-weight: bold; font-size: 16pt; }
        .STYLE180 {color: #FF0000}
        .STYLE186 {font-weight: bold; color: #4f4f4f; }
        .STYLE188 {
            color: #4f4f4f;
            font-family: Arial, Helvetica, sans-serif;
            font-size: 14px;
        }
        .STYLE189 {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 14px;
        }
        .STYLE193 {font-size: 14px}
        .STYLE197 {font-family: Arial, Helvetica, sans-serif}
        .STYLE198 {font-size: 17px; font-family: Arial, Helvetica, sans-serif; }
        .STYLE200 {
            font-size: 14px;
            font-family: Arial, Helvetica, sans-serif;
            color: #4f4f4f;
        }
        .STYLE202 {
            color: #33CC00;
            font-weight: bold;
        }
        .STYLE203 {font-size: 14px; font-family: Arial, Helvetica, sans-serif; color: #4f4f4f; font-weight: bold; }
        .STYLE212 {	color: #0033FF;
            font-family: "Courier New", Courier, monospace;
            font-weight: bold;
            font-size: 16;
        }
        .STYLE216 {color: #0033FF; font-weight: bold; font-size: 16; }
        .STYLE217 {color: #0033FF; font-family: "Courier New", Courier, monospace; }
        .STYLE220 {color: #333333; font-family: Arial, Helvetica, sans-serif; font-size: 14px;}
        .STYLE223 {font-size: 14px; color: #666666; font-family: Arial, Helvetica, sans-serif;}
        .STYLE225 {color: #FF0000; font-size: 16pt; }
        .STYLE227 {font-size: 17px}
        .STYLE228 {color: #008000}
        .STYLE233 {font-family: "华文楷体"}
        .STYLE235 {font-size: 14px; font-weight: bold; }
        .STYLE237 {
            font-size: 16pt;
            color: #000000;
        }
        .STYLE238 {font-size: 24px}
        .STYLE239 {font-size: 18pt}
        .STYLE246 {font-style: italic}
        .STYLE248 {font-weight: bold; color: #333333; font-size: 16pt; }
        .STYLE249 {color: green}
        .STYLE250 {font-weight: bold; color: #4f4f4f; font-size: 16pt; }
        .STYLE251 {color: green; font-weight: bold; font-size: 18pt; }
        .STYLE256 {font-size: 17px; font-family: Arial, Helvetica, sans-serif; font-weight: bold; }
        .STYLE257 {
            color: #6666FF;
            font-weight: bold;
        }
        .STYLE258 {font-size: 18pt; color: #6666FF; }
        .STYLE260 {font-size: 22pt; color: #6666FF; }
        .STYLE262 {color: #6666FF}
        .STYLE263 {color: #0099FF}
        .STYLE265 {
            font-family: "宋体";
            font-size: 12px;
        }
        .STYLE266 {font-family: "华文楷体"; font-size: 14px; }
        .STYLE267 {color: #333333; font-weight: bold; font-size: 14px; font-family: Arial, Helvetica, sans-serif; }
        -->
    </style>
</head>


<body>

<blockquote>
    <table border="0" width=1050>
        <tbody>
        <tr>
            <td width="27%" height="215"><div align="center" class="STYLE90" src="">
                <div align="left"><a href="https://Abige1a.github.io"><img src="./images/xueting.jpg" alt="Xueting Wang" width="223" height="234" hspace="0" vspace="0" border="2" class="STYLE35" /></a></div>
            </div></td>
            <td width="73%"><p><br />
                <span class="STYLE85">
<a href="https://Abige1a.github.io"><font face="Arial" size="5"><b>Xueting Wang </b></font></a></span><a href="https://Abige1a.github.io"><font face="Arial" size="5"><b><span class="STYLE85" lang="zh-cn">王雪婷</span></b></font></a><br />
                <br />


                </a></span><span class="STYLE200"><font face="Arial" size="3">Kate Gleason College of Engineering</font><br />
                    </a></span><span class="STYLE200"><font face="Arial" size="3">Rochester Institute of Technology</font></span><span class="STYLE200"><br />
                    </a></span></p>

                <p><span class="STYLE200"><font face="Arial" size="3">GLE 1565 </font><br />
                    </a></span><span class="STYLE200"><font face="Arial" size="3">74 Lomb Memorial Drive, Rochster, NY, USA 14623</font><br />
                    </a></span></p>
                <p><span class="STYLE200"><font face="Arial" size="3">Email: xu3tingwang@gmail.com</font><br />

</span><br />


                    <a href="https://Abige1a.github.io"><img src="./images/githublogo.jpg" height="30px" style="margin-bottom:-3px"></a>

                    <a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AH8HC4yNOsuDMYz2uxgz_tC61chdLzyzN6li74yIzzdMViKaChUKXpCOKmFWXKMzaReoQnsjJdzVUCyiUL296FEeHJyqvi2h6xMaHww93tIU&user=ZbyTe8cAAAAJ"><img src="./images/GoogleScholar.png" height="30px" style="margin-bottom:-3px"></a>

                    <a href="https://www.youtube.com/@cdimelabs6965"><img src="./images/youtube.png" height="30px" style="margin-bottom:-3px"></a>
                    <a href="https://www.linkedin.com/in/xueting-wang-0929a713a/"><img src="./images/LinkedInlogo.jpg" height="30px" style="margin-bottom:-3px"></a>


                    <br />
                    <br />
                    </span>
                </p></td>
        </tr>
        </tbody>
    </table>




    <b><font face="Arial" size="4">Biography</font></b>

    <table border="1" style="border-width: 0px;" width="1050">
        <tbody>
        <tr>
            <td style="border-style: none; border-width: medium;">
                <p style="margin-top: 3px; margin-bottom: 3px;"><font face="Arial" style="font-size: 12pt;">I am a third-year Ph.D. student in the Department of Industrial and Systems Engineering at the Kate Gleason College of Engineering, Rochester Institute of Technology (RIT), advised by <strong><a href="https://lifeattiktok.com/">Dr. Yunbo Zhang</a></strong>. Prior to joining RIT, I earned my M.S. in Computer and Information Technology from Purdue University and my B.S. in Logistics Management from Southeast University, China.<br>
                    <br>
                    My research focuses on intelligent manufacturing, with an emphasis on human-robot interaction, augmented reality in manufacturing, and long-horizon human-robot collaboration. I am passionate about exploring how seamless collaboration between humans and intelligent robots can drive the future of smart industrial production.
                </font></p>
            </td>
        </tr>
        </tbody>
    </table>


    
    
    <!--<hr />

    <p align="justify"><span class="subtitle1"><span style="font-size: 22pt"><b style="mso-bidi-font-weight: normal"><span
    style="FONT-FAMILY: 'Monotype Corsiva'; COLOR: green; mso-bidi-font-family: Arial"><a name="news" id="news"></a><span class="STYLE258">News</span></span></b></span><span class="STYLE257" style="font-size: 18pt"><span
    style="FONT-FAMILY: 'Bauhaus 93'; mso-bidi-font-family: Arial">
        <o:p></o:p>
      </span></span></span></p>-->


        <br>
        <b><font face="Arial" size="4">News</font></b>

        <div style="overflow-y: scroll; height:150px; width:1100px">
            <table border="1" style="border-width: 0px;" width="1050">
                <tbody>
                <tr>
                    <td style="border-style: none; border-width: medium;">


                        <ul class="STYLE223">
                            <font face="Arial" size="3">
                                
                                <li>05/2025: One paper is accepted to <strong><a href="https://2025.ieeecase.org/">CASE 2025</a></strong>. <a href="https://www.youtube.com/watch?v=BPrWFmKhwG8"> [Video] </a></li>

                                <li>11/2024: I am serving as an organizer for conference <strong><a href="https://spmconf2025.github.io/">Symposium on Solid and Physical Modeling (SPM) 2025</a></strong></li>
                                
                                <li>04/2024: I am nominated by the ISE department to attend the Faculty Women of Color in the Academy National Conference </li>
                                
                                <li>10/2023: One paper is accepted to <strong><a href="https://2024.ieee-icra.org/">ICRA 2024</a></strong>. <a href="https://www.youtube.com/watch?v=mcrLj-tX90s"> [Video] </a> <a href="https://github.com/SherwinDengxiong/contrastive_ICRA24">[Code]</a> </li>
                                
                                <li>04/2023: I am invited to deliver a talk on RIT Industrial and Systems Engineering Department</li>
                                
                                <li>08/2022: I join C-DIME lab supervised by <a href="https://www.rit.edu/directory/ywzeie-yunbo-zhang">Dr. Yunbo Zhang</a></li>

                                

                                
                            </font>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>
        </div>
        <br>





    <b><font face="Arial" size="4">Education</font></b>

    <ul>


        <li ><font size="3" face="Arial">Ph.D. in Industrial and Systems Engineering<br />
            Kate Gleason College of Engineering, Rochester Institute of Technology, NY, USA, August 2022 ~ August 2026<br />
            (Advisor: Prof. <a href="https://www.willyunbozhang.com">Yunbo Zhang</a>)</font></li><font face="Arial" size="3">

        <li>M.S. in Computer and Information Technology<br />
            Purdue University, IN, USA, August 2016 ~ December 2018</li>

        <li>B.S. in Logistics Management<br />
            Southeast University, Nanjing, China, Sep. 2012 ~ Jul. 2016</li>
    </font>
    </ul>

    <b><font face="Arial" size="4">Experiences</font></b>

    <table width="1050" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">
        <tbody>
            

            

            <tr>
            <td width="843">
                <font face="Arial" size="3">

                    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://www.willyunbozhang.com/">Toyota Production Lab</a>, Rochester Institute of Technology, Rochester, USA</p>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; June. 2023 ~ March. 2025</p>


                </font>
            </td>
            <td width="200" align="center">

                <p><img src="images/toyota.png" height="80px"></p>

            </td>
        </tr>

            <tr>
            <td width="843">
                <font face="Arial" size="3">

                    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="https://www.willyunbozhang.com/">CDIME Lab</a>, Rochester Institute of Technology, Rochester, USA</p>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; August. 2022 ~ March. 2025</p>


                </font>
            </td>
            <td width="200" align="center">

                <p><img src="images/CDIME.jpg" height="80px"></p>

            </td>
        </tr>

       


        </tbody>
    </table>

    

    
    <b><font face="Arial" size="4">Teaching</font></b>

        <ul >
            <font face="Arial" size="3">
                

                <span class="style8"><strong><font face="Arial" size="3" color="#808080">Teaching Assistant</font></strong></span>
                <li>ISEE 741: 3D Printing, Fall 2023-Spring,2025</li>

            </font>
    </ul>




        <table width="1180" border="0" cellpadding="1" cellspacing="1">
    <tbody>
        <tr>
            <td colspan="2">
                <p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Conference and Journal Papers</font></strong></span></p>
            </td>
        </tr>

        <!-- Paper 1 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="">
                                    <strong>RoPESim: A Framework for Robot Manipulation Policy Evaluation via Simulation</strong>
                                </a> 
                                <a href="https://www.youtube.com/watch?v=BPrWFmKhwG8">[Video]</a> <br />
                                
                                <strong>Xueting Wang</strong>, Xiwen Dengxiong, Shi Bai, Yunbo Zhang <br />
                                <em>2025 IEEE International Conference on Automation Science and Engineering(<strong>CASE 2025</strong>). (<strong>TOP Conference in Automation</strong>)</em>

                            </p>
                            <p style="font-size: 14px;">
                            Predicting the robot manipulation plan prior to real-world execution is an important capability for robots to complete tasks in manufacturing environments. However, current AI-based manipulation planning methods lack this capability, making it difficult to deploy them in real-world manufacturing scenarios. In this work, we propose a simulation-based human-robot collaboration framework to evaluate predicted robot actions before real-world execution. The framework consists of a VLM-based scenario generator, a diffusion-based action simulator, and an evaluator. First, the scenario generator automatically creates a simulation scenario with objects and obstacles identified and placed. Then, the action simulator generates a series of manipulation action trajectories using a diffusion model in the simulation environment. Each action trajectory is assessed by the evaluator for collision failure, manipulation failure, and completion rate. The final evaluation results are returned to the user for verification and approval. In our experiment, we apply our framework to five chosen scenarios with highly potential collision failures. For each scenario, at least one feasible planned action trajectory is generated. It is then verified through real robot execution, demonstrating the effectiveness of the proposed framework.
                            </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/CASE25.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>

        <!-- Paper 2 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="">
                                    <strong>VLAbot: A Human–Vision–Language–Action Models Interaction Framework for Robotic Assembly</strong>
                                </a> <br />
                                
                                <strong>Xueting Wang</strong>, Xiwen Dengxiong, Shi Bai, Yunbo Zhang <br />
                                <em>Robotics and Computer-Integrated Manufacturing (under review) May 2025. (<strong>TOP Journal in Manufacturing</strong>)</em>
                            </p>

                        <p style="font-size: 14px;">
                            We propose an intelligent human-robot collaboration system designed to assist embodied intelligence in learning complex, long-horizon manufacturing assembly tasks. The system integrates multiple expert agents and augmented reality (AR) interaction interfaces, allowing robots to request planning and execution guidance from humans and efficiently complete complex tasks. Specifically, the expert agents, equipped with visual language models and large language models, actively interact with users through text, vision, and action modalities to acquire critical information, learn task-specific skills, and develop strategies for sub-task planning. A distributed data and model architecture ensures real-time interactions between different models and facilitates seamless collaboration between humans and robots. We evaluate the system on two challenging long-horizon manufacturing assembly tasks—gear assembly and peg insertion—to demonstrate the effectiveness of the proposed human-robot collaboration approach. The intelligent human-robot collaboration system successfully learns the two assembly tasks within five trials and enabled the embodied intelligence to accomplish two tasks within 8 minutes.
                        </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/RCIM25.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>


        <!-- Paper 3 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="">
                                    <strong>HALO: Hierarchical Long-horizon Manipulation with Failure Recovery</strong>
                                </a> 
                                <a href="https://www.youtube.com/watch?v=9Uu5JLlKcFk">[Video]</a> <br />
                                
                                <strong>Xueting Wang</strong>, Xiwen Dengxiong, Shi Bai, Yunbo Zhang <br />
                                <em>2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS 2025</strong>)(under review). (<strong>TOP Conference in Robotics</strong>)</em>
                            </p>
                            <p style="font-size: 14px;">
                                Finding a backtracking solution based on executed tasks when unexpected failure happens is a promising approach to assist robots in completing long and complex tasks. However, it is still unclear how to dynamically make the manipulation planning for the long-horizon task and finalize the appropriate backtracking strategies when failure arises. In this work, we propose an interactive task planning system that is integrated with a hierarchical task planning agent for long-horizon robot manipulation. The key features of the system are (1) dynamically identifying the failure and constructing the hierarchical manipulation plans for long-horizon tasks and (2) generating appropriate failure recovery solutions through human-robot interaction. In the experiment, we design two challenging long-horizon tasks, a vehicle chassis assembly task, and a hot dog serving task, to demonstrate the effectiveness of the system. The results indicate that the proposed system successfully generates the backtracking strategy and enhances the completeness rate of the long-horizon task.
                            </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/IROSrecovery.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>

        <!-- Paper 4 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="">
                                    <strong>Zero-shot Robot Manipulation via Action Decomposition and Composition</strong>
                                </a> 
                                <a href="https://www.youtube.com/watch?v=SqYt3lxj0o8">[Video]</a> <br />
                                
                                Xiwen Dengxiong, <strong>Xueting Wang</strong>, Rui Li, Yunbo Zhang <br />
                                <em>2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (<strong>IROS 2025</strong>)(under review). (<strong>TOP Conference in Robotics</strong>)</em>
                            </p>

                            <p style="font-size: 14px;">
                                The ability to learn generalized skills from demonstrations and apply the acquired skills in various real-world scenarios is a key challenge for robot manipulation. Different from the typical robot learning tasks that learn the action from multiple demonstrated samples in a single task, zero-shot robot manipulation requires the robot to efficiently leverage multiple learned robot skills to accomplish a new task. In this paper, we propose an action decomposition/composition framework that efficiently transfers key manipulation skills to various new derivative tasks. Specifically, we first decompose one demonstration that encompasses several foundation skills that do not contain the derivative task. Then we adopt an action prediction approach to generate possible manipulation poses and the end pose for each subtask in the derivative task based on the robot's action and the video frames from the robot cameras. Since the generated poses may be impacted by previous misleading actions, we denoise the action by selecting the most possible manipulation poses based on the task to guide the robot manipulation. In the experiment, we show our framework can manipulate both robotic arm and automated Guided Vehicle (AGV) in the derivative task.
                            </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/IROSlearning.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>

        <!-- Paper 5 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="">
                                    <strong>LLM-ResiGame: Multi-Agent Large Language Models for Creating Scenario-Based Resilience Games in Critical Infrastructure Decision-Making Practices</strong>
                                </a> <br />
                                Huadong Zhang, <strong>Xueting Wang</strong>, Xiwen Dengxiong, Xuhang Yuan, Justus Robertson, Yunbo Zhang, David Schwartz, Chao Peng<br />

                                <em></em>
                            </p>
                                Resilience training for critical infrastructure decision-making is essential for preparing against both cybersecurity threats and natural disasters. Traditional training methods often lack flexibility in scenario customization, limiting their effectiveness in simulating complex crisis environments. Recent advancements in agentic AI have shown that it can solve complex problems that require multiple steps. In this work, we present LLM-ResiGame, a multi-agent large language model (LLM)-driven resilience game that dynamically generates scenario-based challenges involving cyberattacks and natural disasters. The game framework enables players to manage critical infrastructure, allocate resources, and respond to evolving crises through interactive decision-making. We evaluate the content quality, user experience, and game engagement of our approach through a user study where players play with customized pre-generated scenarios and real-time events. We found that users were able to successfully configure and adapt scenarios, and that our multi-agent LLM system maintains scenario relevance while balancing the game elements and components. These findings highlight key benefits and challenges in using LLM-driven multi-agent systems for enhancing scenario-based resilience training in critical infrastructure management.
                            <p>
                                
                            </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/LLM-ResiGame.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>


        <!-- Paper 6 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="https://arxiv.org/abs/2404.03067">
                                    <strong>Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented Reality Teleoperation System</strong>
                                </a> 
                                <a href="https://www.youtube.com/watch?v=mcrLj-tX90s">[Video]</a> 
                                <a href="https://github.com/SherwinDengxiong/contrastive_ICRA24">[Code]</a><br />
                                Xiwen Dengxiong, <strong>Xueting Wang</strong>, Shi Bai, Yunbo Zhang <br />
                                <em>2024 IEEE International Conference on Robotics and Automation (<strong>ICRA 2024</strong>), Yokohama, Japan, May 2024.(<strong>TOP Conference in Robotics</strong>)</em>
                            </p>

                            <p style="font-size: 14px;">
                                Most existing 6-DoF robot grasping solutions depend on strong supervision on grasp pose to ensure satisfactory performance, which could be laborious and impractical when the robot works in some restricted area. To this end, we propose a self-supervised 6-DoF grasp pose detection framework via an Augmented Reality (AR) teleoperation system that can efficiently learn human demonstrations and provide 6-DoF grasp poses without grasp pose annotations. Specifically, the system collects the human demonstration from the AR environment and contrastively learns the grasping strategy from the demonstration. For the real-world experiment, the proposed system leads to satisfactory grasping abilities and learning to grasp unknown objects within three demonstrations.
                            </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/ICRA24.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>



        <!-- Paper 7 -->
        <tr>
            <td width="50%">
                <font size="3" face="Arial">
                    <li>
                        <div align="justify">
                            <p>
                                <a href="https://doi.org/10.1115/1.4062658">
                                    <strong>“I can see your password”: A Case Study about Cybersecurity Risks in Mid-air Interactions of Mixed reality-based Smart Manufacturing Applications</strong>
                                </a> <br />
                                Wenhao Yang, Xiwen Dengxiong, <strong>Xueting Wang</strong>, Yidan Hu, Yunbo Zhang
                                <em>Journal of Computing and Information Science in Engineering (<strong>JCISE</strong>), published on Mar 2024. (<strong>TOP Journal in Manufacturing</strong>)</em>
            
                                <em></em>
                            </p>

                            <p style="font-size: 14px;">
                                This paper aims to present a potential cybersecurity risk existing in mixed reality (MR)-based smart manufacturing applications that decipher digital passwords through a single RGB camera to capture the user’s mid-air gestures. We first created a test bed, which is an MR-based smart factory management system consisting of mid-air gesture-based user interfaces (UIs) on a video see-through MR head-mounted display. To interact with UIs and input information, the user’s hand movements and gestures are tracked by the MR system. We setup the experiment to be the estimation of the password input by users through mid-air hand gestures on a virtual numeric keypad. To achieve this goal, we developed a lightweight machine learning-based hand position tracking and gesture recognition method. This method takes either video streaming or recorded video clips (taken by a single RGB camera in front of the user) as input, where the videos record the users’ hand movements and gestures but not the virtual UIs. With the assumption of the known size, position, and layout of the keypad, the machine learning method estimates the password through hand gesture recognition and finger position detection. The evaluation result indicates the effectiveness of the proposed method, with a high accuracy of 97.03%, 94.06%, and 83.83% for 2-digit, 4-digit, and 6-digit passwords, respectively, using real-time video streaming as input with known length condition. Under the unknown length condition, the proposed method reaches 85.50%, 76.15%, and 77.89% accuracy for 2-digit, 4-digit, and 6-digit passwords, respectively
                            </p>
                        </div>
                    </li>
                </font>
            </td>
            <td width="50%" align="center">
                <img src="images/JCISE24.jpg" alt="Paper 1 Image" width="100%">
                <img src="images/JCISE24.png" alt="Paper 1 Image" width="100%">
            </td>
        </tr>
        

    </tbody>
</table>


        
        <!--<hr />

        <p></p>-->



        <!--    <table width="1180" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">-->
        <!--        <tbody>-->
        <!--        <tr>-->
        <!--            <td height="128" ><ol>-->



        <!--                <p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Workshop Papers</font></strong></span></p>-->
        <!--    -->
        <!--                <font face="Arial" size="3">-->
        <!--    -->
        <!--                    <li><div align="justify"><p>-->
        <!--                        <strong>Generatively Inferential Co-Training for Unsupervised Domain Adaptation</strong> <br />-->
        <!--                        Can Qin, Lichen Wang, <strong>Yulun Zhang</strong>, Yun Fu<br />-->
        <!--                        <em>ICCV Real-World Recognition from Low-Quality Images and Videos workshop (<strong>ICCV RLQ</strong>), Seoul, Korea, Oct./Nov. 2019. </em>(<strong>Oral</strong>) (<strong><a href="awards/ICCW_Award.jpeg">Best Paper Award</a></strong>)<br />-->
        <!--                        [<a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/RLQ/Qin_Generatively_Inferential_Co-Training_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.pdf">PDF</a>] [<a href="https://github.com/ChinTsan01/Generatively-Inferential-Co-Training-for-UDA">Code</a>]-->
        <!--                    </p></div></li>-->
        <!--    -->
        <!--                    <li><div align="justify"><p>-->
        <!--                        <strong>NTIRE 2017 Challenge on Single Image-->
        <!--                            Super-Resolution: Methods and Results </strong> <br />-->
        <!--                        Radu Timofte, Eirikur Agustsson, Luc Van Gool, ..., Xintao Wang, Yapeng Tian, Ke Yu, <strong>Yulun Zhang</strong>, Shixiang Wu, Chao Dong, Liang Lin, Yu Qiao, ..., et al. <br />-->
        <!--                        <em>IEEE CVPR New Trends in Image Restoration-->
        <!--                            and Enhancement workshop and challenge on image super-resolution (<strong>CVPR NTIRE</strong>), Hawaii, USA, Jul. 2017. </em>(<strong><a href="http://yulunzhang.com/img/CVPR17NTIRE-SecondAward.jpg">Second Place Award</a></strong>)<br />-->
        <!--                        [<a href="papers/Timofte-CVPRW-2017.pdf">PDF</a>]</p></div></li>-->
        <!--    -->
        <!--                </font>-->
        <!--            </ol></td>-->
        <!--        </tr>-->
        <!--        </tbody>-->
        <!--    </table>-->


        <!--    <table width="1180" height="130" border="0" cellpadding="1" cellspacing="1" bordercolor="#FF0000">-->
        <!--        <tbody>-->




        <!--        <tr>-->
        <!--            <td height="128" ><ol>-->
        <!--    -->
        <!--                <p><span class="style8"><strong><font face="Arial" size="3" color="#808080">Preprints</font></strong></span></p>-->
        <!--    -->
        <!--                <font size="3" face="Arial">-->
        <!--    -->
        <!--                    <li><div align="justify"><p>-->
        <!--                        <strong>Pyramid Attention Networks for Image Restoration</strong> <br />-->
        <!--                        Yiqun Mei, Yuchen Fan, <strong>Yulun Zhang</strong>, Jiahui Yu, Yuqian Zhou, Ding Liu, Yun Fu, Thomas S. Huang, Humphrey Shi <br />-->
        <!--                        <em>arXiv preprint arXiv:2004.13824, 2020.</em><br />-->
        <!--                        [<a href="https://arxiv.org/pdf/2004.13824.pdf">PDF</a>] [<a href="https://github.com/SHI-Labs/Pyramid-Attention-Networks">Code</a>]</p></div></li>-->
        <!--    -->
        <!--
        <li><div align="justify"><p>
        <strong>Neural Sparse Representation for Image Restoration</strong> <br />
        Yuchen Fan, Jiahui Yu, Yiqun Mei, <strong>Yulun Zhang</strong>, Yun Fu, Ding Liu, Thomas S Huang <br />
        <em>arXiv preprint arXiv:2006.04357, 2020.</em><br />
        [<a href="https://arxiv.org/pdf/2006.04357.pdf">PDF</a>] [Code]</p></div></li>
        -->


        <!--                </font>-->
        <!--            </ol></td>-->
        <!--        </tr>-->
        <!--    -->
        <!--        </tbody>-->
        <!--    </table>-->


        <!--    <b><font face="Arial" size="4">Awards</font></b>-->
        <!--    -->
        <!--    <ul>-->
        <!--        <font face="Arial" size="3">-->
        <!--            <li><a href="http://yulunzhang.com/awards/ICCW_Award.jpeg">Best Paper Award</a>, RLQ workshop, IEEE ICCV,  2019</li>-->
        <!--            <li>ICCV Travel Award,  2019</li>-->
        <!--            <li>ICLR Travel Award,  2019</li>-->
        <!--            <li>PhD Network Travel Grant, Northeastern University, USA, 2018, 2019</li>-->
        <!--            <li>Dean's Fellowship, Northeastern University, USA, 2017</li>-->
        <!--            <li><a href="http://suisf.sz.edu.cn/20170713gg.htm">Shenzhen Universiade International Scholarship</a>, 2017</li>-->
        <!--            <li>Excellent Graduate of Beijing,  Beijing, 2017</li>-->
        <!--            <li>Excellent Graduate in Department of Automation,  Tsinghua University, 2017</li>-->
        <!--            <li>Excellent Master Thesis Award, Tsinghua University, 2017</li>-->
        <!--            <li><a href="http://yulunzhang.com/img/CVPR17NTIRE-SecondAward.jpg">Second Place Award</a>, NTIRE workshop, IEEE CVPR,  2017</li>-->
        <!--            <li>National Scholarship (Ministry of Education, China, Top 2%), 2016</li>-->
        <!--            <li><a href="http://yulunzhang.com/img/VCIP2015best student paper-web.jpg">Best Student Paper Award</a> at IEEE Visual Communications and Image Processing (VCIP), 2015</li>-->
        <!--            <li>Jingzhi Research Award in Tsinghua University, 2015</li>-->
        <!--        </font>-->
        <!--    </ul>-->




        <!--    <b><font face="Arial" size="4">Academic Service</font></b>-->
        <!--    -->
        <!--    <ul>-->
        <!--        <font face="Arial" size="3">-->


        <!--            <span class="style8"><strong><font face="Arial" size="3" color="#808080">Area Chair or Senior Program Committee</font></strong></span>-->
        <!--            <li>International Joint Conferences on Artificial Intelligence (IJCAI), 2021</li>-->
        <!--            &lt;!&ndash;<li>CAAI International Conference on Artificial Intelligence (CICAI), 2021</li>&ndash;&gt;-->
        <!--    -->
        <!--            <br>-->
        <!--            <span class="style8"><strong><font face="Arial" size="3"><strong><font face="Arial" size="3" color="#808080">Program Committee</font></strong></font></strong></span><strong><font color="#808080"> or</font></strong><span class="style8"><strong><font face="Arial" size="3" color="#808080"> Reviewer</font></strong></span>-->
        <!--    -->
        <!--            <li>Conference on Computer Vision and Pattern Recognition (CVPR), 2019-2021</li>-->
        <!--            <li>International Conference on Computer Vision (ICCV), 2019-2021</li>-->
        <!--            <li>European Conference on Computer Vision (ECCV), 2020</li>-->
        <!--            <li>International Conference on Learning Representations (ICLR), 2020-2022</li>-->
        <!--            <li>Neural Information Processing Systems (NeurIPS), 2020-2021</li>-->
        <!--            <li>International Conference on Machine Learning (ICML), 2021</li>-->
        <!--            <li>AAAI Conference on Artificial Intelligence (AAAI), 2019-2022</li>-->
        <!--            <li>International Joint Conferences on Artificial Intelligence (IJCAI), 2020-2024</li>-->
        <!--            <li>ACM International Conference on Multimedia (ACM MM), 2021</li>-->
        <!--            <li>International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2019-2021</li>-->
        <!--            &lt;!&ndash;-->
        <!--            <li>CVPR 2019, 2020</li>-->
        <!--            <li>ICCV 2019</li>-->
        <!--            <li>ECCV 2020</li>-->
        <!--            <li>NeurIPS 2020</li>-->
        <!--            <li>AAAI 2019, 2020</li>-->
        <!--            <li>IJCAI 2020</li>-->
        <!--            <li>MICCAI 2020</li>-->
        <!--            &ndash;&gt;-->
        <!--            <br>-->
        <!--            <span class="style8"><strong><font face="Arial" size="3" color="#808080">Journal Reviewer</font></strong></span>-->
        <!--            <li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</a></li>-->
        <!--            <li><a href="https://www.springer.com/journal/11263">International Conference on Computer Vision (IJCV)</a></li>-->
        <!--            <li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE Transactions on Image Processing (TIP)</a></li>-->
        <!--            <li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</a></li>-->
        <!--            <li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IEEE Transactions on Multimedia (TMM)</a></li>-->
        <!--            <li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE Transactions on Circuits System and Video Technology (TCSVT)</a></li>-->
        <!--            <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036">IEEE Transactions on Cybernetics (TCYB)</a></li>-->
        <!--            <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36">IEEE Transactions on Geoscience and Remote Sensing (TGRS)</a></li>-->
        <!--            <li><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852">IEEE Transactions on Computational Imaging (TCI)</a></li>-->
        <!--            <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42">IEEE Transactions on Medical Imaging (TMI)</a></li>-->
        <!--            <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=7274989">IEEE Transactions on Cognitive and Developmental Systems (TCDS)</a></li>-->
        <!--            <li><a href="https://dl.acm.org/journal/tist">ACM Transactions on Intelligent Systems and Technology (TIST)</a></li>-->
        <!--            <li><a href="https://www.journals.elsevier.com/pattern-recognition">Pattern Recognition (PR)</a></li>-->
        <!--            <li><a href="https://www.sciencedirect.com/journal/information-fusion">Information Fusion (INFFUS)</a></li>-->
        <!--            <li><a href="https://ees.elsevier.com/cviu/">Computer Vision and Image Understanding (CVIU)</a></li>-->
        <!--        </font>-->
        <!--    </ul>-->
        <!--
        <span class="STYLE178" style="font-family: 'Monotype Corsiva'">-</span><span class="STYLE248" style="font-family: 'Monotype Corsiva'">Attend Conferences (Oral or Poster) and Invited Talks </span> </blockquote>

        <blockquote>
          <ul type="square">
            <li class="STYLE200"><strong>Invited Talk:</strong> Adaptive Local Nonparametric Regression for Fast Single Image Super‐Resolution, Singapore, Dec. 2015 </li>
            <li class="STYLE200"><strong>Invited Talk:</strong> Single Image Super-Resolution via Iterative Collaborative Representation, Gwangju, Korea, Sep. 2015 </li>
            <li class="STYLE200"><strong>Invited Talk:</strong> Single Depth Image Super-Resolution via A Dual Sparsity Model, Torino, Italy, Jun. 2015 </li>
            <li class="STYLE200">Image Super-Resolution based on Dictionary Learning and Anchored Neighborhood Regression with Mutual Inconherence, ICIP2015, Quebec, Canada, Sep. 2015 </li>
          </ul>
        <span class="STYLE248" style="font-family: 'Monotype Corsiva'">-Memberships</span></blockquote>
        <blockquote>
          <ul type="square">
            <li class="STYLE200">Student Member, IEEE</li>
            <li class="STYLE200">Student Member, CCF
              <span style="font-size: 22pt"><span class="STYLE257" style="font-size: 18pt"><span
        style="FONT-FAMILY: 'Bauhaus 93'; mso-bidi-font-family: Arial">
              <o:p></o:p>
              </span></span></span><br />
            </li>
          </ul>
        -->
        <!--<hr />-->



        <!--<hr />-->

        

        

        <table cellpadding="0px">
	<tbody>
		<tr>
			<td width="360px">
			<div id="clustrmaps-widget"></div>
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=5gTHXXHEOWgCn4CTTwz2Y-XrInhUEXEmQT4BZNpFm3c&cl=ffffff&w=a"></script>
			<div class="jvectormap-tip"></div>
			</td>
			<td valign="top">
				<div style="clear:both;">
				<p align="right"><font size="2">Last Updated on May 27, 2025<br>
				Published with <a href="https://pages.github.com/">GitHub Pages</a></font></p>
				</div>
				<div style="clear: both;">
				<p align="right"><font size="5"><img style="width: 80px;" src="images/beaver.gif"></font></p>
				</div>
			</td>
		</tr>
	</tbody>
</table>
        

        
        <!--    <b><font face="Arial" size="4">Links</font></b>-->
        <!--    -->
        <!--    <ul >-->
        <!--        <font face="Arial" size="3">-->
        <!--            <li>Collaborators: <a href="http://media.au.tsinghua.edu.cn/qhdai.html" target="_blank" rel="nofollow">Qionghai Dai</a> (THU), <a href="https://jianzhang.tech/" target="_blank" rel="nofollow">Jian Zhang</a> (PKU), <a href="http://yapengtian.org/" target="_blank" rel="nofollow">Yapeng Tian</a> (U of R), <a href="https://sites.google.com/site/kunpengli1994/" target="_blank" rel="nofollow">KunpengLi</a> (NEU), <a href="http://kailigo.github.io/" target="_blank" rel="nofollow">Kai Li</a> (NEU), <a href="https://chintsan01.github.io/" target="_blank" rel="nofollow">Can Qin</a> (NEU)<br /></li>-->
        <!--            <li><a href="https://openaccess.thecvf.com/menu">CVF</a></li>-->
        <!--            <li>Journal impact: <a href="papers/journal_impact/2018JournalImpactFactor.pdf">2018</a>, <a href="papers/journal_impact/2019JournalImpactFactor.pdf">2019</a>, <a href="papers/journal_impact/2020JournalImpactFactor.pdf">2020</a></li>-->
        <!--            <li>Top publications: <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_computervisionpatternrecognition">Computer Vision & Pattern Recognition</a>, <a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence">Artificial Intelligence</a></li>-->
        <!--            <li>Resources: <a href="https://github.com/YapengTian/Single-Image-Super-Resolution">Image Super-Resolution</a></li>-->
        <!--            <li>Resources: <a href="https://github.com/yulunzhang/video-enhancement">Video Enhancement</a></li></font>-->


        <!--    </ul>-->


        

        <p class="STYLE162">


            <br>
            <!-- Start of StatCounter Code for Default Guide -->
            <!--

            <script type="text/javascript">
            var sc_project=11376711;
            var sc_invisible=0;
            var sc_security="f65f77d8";
            var scJsHost = (("https:" == document.location.protocol) ?
            "https://secure." : "http://www.");
            document.write("<sc"+"ript type='text/javascript' src='" +
            scJsHost+
            "statcounter.com/counter/counter.js'></"+"script>");
            </script>
            <noscript>
            </noscript>
            <noscript><div class="statcounter"><a title="web analytics"
            href="http://statcounter.com/" target="_blank"><img
            class="statcounter" style="width: 0px;"
            src="//c.statcounter.com/11376711/0/f65f77d8/0/" alt="web
            analytics"></a></div></noscript>

            -->
            <!-- End of StatCounter Code for Default Guide -->
            <a href="https://clustrmaps.com/site/19ncr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&cl=ffffff" style="width: 0px;" /></a>
</blockquote>
</body>
</html>
<!--Footer-->
<!--© 2022 GitHub, Inc.-->
<!--Footer navigation-->
<!--Terms-->
<!--Privacy-->
<!--Security-->
<!--Status-->
<!--Docs-->
<!--Contact GitHub-->
<!--Pricing-->
<!--API-->
<!--Training-->
<!--Blog-->
<!--About-->
<!--SherwinDengxiong.github.io/index.html at main · SherwinDengxiong/SherwinDengxiong.github.io-->